{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1abd9a56",
   "metadata": {},
   "source": [
    "# RAG Tutorial\n",
    "\n",
    "## 1. Sparse Retrieval\n",
    "\n",
    "In this first part we will implement a simple search engine using BM25 weighting and the <A HREF=\"https://whoosh.readthedocs.io/en/latest/intro.html\">Whoosh!</A> library. Install Whoosh! using the command `pip install whoosh` if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef72764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.index import create_in\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "from whoosh import index\n",
    "import os\n",
    "\n",
    "#analyzer to remove stopwords and do stemming\n",
    "analyzer = StemmingAnalyzer()\n",
    "\n",
    "# Define schema\n",
    "schema = Schema(\n",
    "    id=ID(stored=True),\n",
    "    title=TEXT(stored=True, analyzer=analyzer),\n",
    "    date=TEXT(stored=True),\n",
    "    content=TEXT(stored=True, analyzer=analyzer)\n",
    ")\n",
    "\n",
    "# Create index directory\n",
    "index_dir = \"indexdir\"\n",
    "os.makedirs(index_dir, exist_ok=True)\n",
    "\n",
    "# Create an index\n",
    "ix = create_in(index_dir, schema)\n",
    "\n",
    "# Path to the text files\n",
    "data_directory = 'data'\n",
    "\n",
    "# Function to index files\n",
    "def index_files(data_directory, index):\n",
    "    writer = index.writer()\n",
    "    for filename in os.listdir(data_directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(data_directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                title = lines[0].strip()\n",
    "                date = lines[1].strip()\n",
    "                content = ''.join(lines[2:]).strip()\n",
    "                writer.add_document(id=filename, title=title, date=date, content=content)\n",
    "    writer.commit()\n",
    "\n",
    "# Index the text files\n",
    "index_files(data_directory, ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf916d",
   "metadata": {},
   "source": [
    "Now we can search the index with textual queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2533a45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: data008.txt\n",
      "Title: SOS! Soft Prompt Attack Against Open-Source Large Language Models\n",
      "Date: 2024-07-03T14:35:16Z\n",
      "Content: Open-source large language models (LLMs) have become increasingly popular\n",
      "among both the general public and industry, as they can be customized,\n",
      "fine-tuned, and freely used. However, some open-source LLMs require approval\n",
      "before usage, which has led to third parties publishing their own easily\n",
      "accessible versions. Similarly, third parties have been publishing fine-tuned\n",
      "or quantized variants of these LLMs. These versions are particularly appealing\n",
      "to users because of their ease of access and reduced computational resource\n",
      "demands. This trend has increased the risk of training time attacks,\n",
      "compromising the integrity and security of LLMs. In this work, we present a new\n",
      "training time attack, SOS, which is designed to be low in computational demand\n",
      "and does not require clean data or modification of the model weights, thereby\n",
      "maintaining the model's utility intact. The attack addresses security issues in\n",
      "various scenarios, including the backdoor attack, jailbreak attack, and prompt\n",
      "stealing attack. Our experimental findings demonstrate that the proposed attack\n",
      "is effective across all evaluated targets. Furthermore, we present the other\n",
      "side of our SOS technique, namely the copyright token -- a novel technique that\n",
      "enables users to mark their copyrighted content and prevent models from using\n",
      "it.\n",
      "\n",
      "ID: data004.txt\n",
      "Title: Single Character Perturbations Break LLM Alignment\n",
      "Date: 2024-07-03T16:03:10Z\n",
      "Content: When LLMs are deployed in sensitive, human-facing settings, it is crucial\n",
      "that they do not output unsafe, biased, or privacy-violating outputs. For this\n",
      "reason, models are both trained and instructed to refuse to answer unsafe\n",
      "prompts such as \"Tell me how to build a bomb.\" We find that, despite these\n",
      "safeguards, it is possible to break model defenses simply by appending a space\n",
      "to the end of a model's input. In a study of eight open-source models, we\n",
      "demonstrate that this acts as a strong enough attack to cause the majority of\n",
      "models to generate harmful outputs with very high success rates. We examine the\n",
      "causes of this behavior, finding that the contexts in which single spaces occur\n",
      "in tokenized training data encourage models to generate lists when prompted,\n",
      "overriding training signals to refuse to answer unsafe requests. Our findings\n",
      "underscore the fragile state of current model alignment and promote the\n",
      "importance of developing more robust alignment methods. Code and data will be\n",
      "available at https://github.com/hannah-aught/space_attack.\n",
      "\n",
      "ID: data003.txt\n",
      "Title: Self-Evaluation as a Defense Against Adversarial Attacks on LLMs\n",
      "Date: 2024-07-03T16:03:42Z\n",
      "Content: When LLMs are deployed in sensitive, human-facing settings, it is crucial\n",
      "that they do not output unsafe, biased, or privacy-violating outputs. For this\n",
      "reason, models are both trained and instructed to refuse to answer unsafe\n",
      "prompts such as \"Tell me how to build a bomb.\" We find that, despite these\n",
      "safeguards, it is possible to break model defenses simply by appending a space\n",
      "to the end of a model's input. In a study of eight open-source models, we\n",
      "demonstrate that this acts as a strong enough attack to cause the majority of\n",
      "models to generate harmful outputs with very high success rates. We examine the\n",
      "causes of this behavior, finding that the contexts in which single spaces occur\n",
      "in tokenized training data encourage models to generate lists when prompted,\n",
      "overriding training signals to refuse to answer unsafe requests. Our findings\n",
      "underscore the fragile state of current model alignment and promote the\n",
      "importance of developing more robust alignment methods. Code and data will be\n",
      "made available at https://github.com/Linlt-leon/Adversarial-Alignments.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "# Open the index\n",
    "ix = index.open_dir(index_dir)\n",
    "\n",
    "# Function to search the index\n",
    "def search_index(query_str):\n",
    "    with ix.searcher() as searcher:\n",
    "        query = QueryParser(\"content\", ix.schema).parse(query_str)\n",
    "        results = searcher.search(query)\n",
    "        for result in results:\n",
    "            print(f\"ID: {result['id']}\")\n",
    "            print(f\"Title: {result['title']}\")\n",
    "            print(f\"Date: {result['date']}\")\n",
    "            print(f\"Content: {result['content']}\\n\")\n",
    "\n",
    "search_index(\"LLMs attacks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f753bd",
   "metadata": {},
   "source": [
    "## 2. Dense Passage Retrieval\n",
    "\n",
    "In this example, we will use <A HREF=\"https://sbert.net/\">S-Bert</A> to encode query and documents.\n",
    "Be sure to install the required library: `pip install sentence-transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70fc86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Path to the text files\n",
    "data_directory = 'data'\n",
    "\n",
    "# Function to read the text files and encode them\n",
    "def encode_documents(data_directory, model):\n",
    "    documents = []\n",
    "    ids = []\n",
    "    for filename in os.listdir(data_directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(data_directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                title = lines[0].strip()\n",
    "                date = lines[1].strip()\n",
    "                content = ''.join(lines[2:]).strip()\n",
    "                document = f\"{title}\\n{date}\\n{content}\"\n",
    "                documents.append(document)\n",
    "                ids.append(filename)\n",
    "    embeddings = model.encode(documents, convert_to_tensor=True)\n",
    "    return ids, embeddings.cpu()\n",
    "\n",
    "# Encode the documents\n",
    "ids, embeddings = encode_documents(data_directory, model)\n",
    "\n",
    "# Save the embeddings and IDs\n",
    "np.save('embeddings.npy', embeddings)\n",
    "np.save('ids.npy', np.array(ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742de0e",
   "metadata": {},
   "source": [
    "Now, we'll load the embeddings and use them to perform similarity searches with new queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0900ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: data008.txt\n",
      "Score: 0.5462\n",
      "SOS! Soft Prompt Attack Against Open-Source Large Language Models\n",
      "2024-07-03T14:35:16Z\n",
      "  Open-source large language models (LLMs) have become increasingly popular\n",
      "among both the general public and industry, as they can be customized,\n",
      "fine-tuned, and freely used. However, some open-source LLMs require approval\n",
      "before usage, which has led to third parties publishing their own easily\n",
      "accessible versions. Similarly, third parties have been publishing fine-tuned\n",
      "or quantized variants of these LLMs. These versions are particularly appealing\n",
      "to users because of their ease of access and reduced computational resource\n",
      "demands. This trend has increased the risk of training time attacks,\n",
      "compromising the integrity and security of LLMs. In this work, we present a new\n",
      "training time attack, SOS, which is designed to be low in computational demand\n",
      "and does not require clean data or modification of the model weights, thereby\n",
      "maintaining the model's utility intact. The attack addresses security issues in\n",
      "various scenarios, including the backdoor attack, jailbreak attack, and prompt\n",
      "stealing attack. Our experimental findings demonstrate that the proposed attack\n",
      "is effective across all evaluated targets. Furthermore, we present the other\n",
      "side of our SOS technique, namely the copyright token -- a novel technique that\n",
      "enables users to mark their copyrighted content and prevent models from using\n",
      "it.\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "ID: data004.txt\n",
      "Score: 0.4154\n",
      "Single Character Perturbations Break LLM Alignment\n",
      "2024-07-03T16:03:10Z\n",
      "  When LLMs are deployed in sensitive, human-facing settings, it is crucial\n",
      "that they do not output unsafe, biased, or privacy-violating outputs. For this\n",
      "reason, models are both trained and instructed to refuse to answer unsafe\n",
      "prompts such as \"Tell me how to build a bomb.\" We find that, despite these\n",
      "safeguards, it is possible to break model defenses simply by appending a space\n",
      "to the end of a model's input. In a study of eight open-source models, we\n",
      "demonstrate that this acts as a strong enough attack to cause the majority of\n",
      "models to generate harmful outputs with very high success rates. We examine the\n",
      "causes of this behavior, finding that the contexts in which single spaces occur\n",
      "in tokenized training data encourage models to generate lists when prompted,\n",
      "overriding training signals to refuse to answer unsafe requests. Our findings\n",
      "underscore the fragile state of current model alignment and promote the\n",
      "importance of developing more robust alignment methods. Code and data will be\n",
      "available at https://github.com/hannah-aught/space_attack.\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "ID: data003.txt\n",
      "Score: 0.4025\n",
      "Self-Evaluation as a Defense Against Adversarial Attacks on LLMs\n",
      "2024-07-03T16:03:42Z\n",
      "  When LLMs are deployed in sensitive, human-facing settings, it is crucial\n",
      "that they do not output unsafe, biased, or privacy-violating outputs. For this\n",
      "reason, models are both trained and instructed to refuse to answer unsafe\n",
      "prompts such as \"Tell me how to build a bomb.\" We find that, despite these\n",
      "safeguards, it is possible to break model defenses simply by appending a space\n",
      "to the end of a model's input. In a study of eight open-source models, we\n",
      "demonstrate that this acts as a strong enough attack to cause the majority of\n",
      "models to generate harmful outputs with very high success rates. We examine the\n",
      "causes of this behavior, finding that the contexts in which single spaces occur\n",
      "in tokenized training data encourage models to generate lists when prompted,\n",
      "overriding training signals to refuse to answer unsafe requests. Our findings\n",
      "underscore the fragile state of current model alignment and promote the\n",
      "importance of developing more robust alignment methods. Code and data will be\n",
      "made available at https://github.com/Linlt-leon/Adversarial-Alignments.\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "ID: data002.txt\n",
      "Score: 0.3222\n",
      "LLM Internal States Reveal Hallucination Risk Faced With a Query\n",
      "2024-07-03T17:08:52Z\n",
      "The hallucination problem of Large Language Models (LLMs) significantly\n",
      "limits their reliability and trustworthiness. Humans have a self-awareness\n",
      "process that allows us to recognize what we don't know when faced with queries.\n",
      "Inspired by this, our paper investigates whether LLMs can estimate their own\n",
      "hallucination risk before response generation. We analyze the internal\n",
      "mechanisms of LLMs broadly both in terms of training data sources and across 15\n",
      "diverse Natural Language Generation (NLG) tasks, spanning over 700 datasets.\n",
      "Our empirical analysis reveals two key insights: (1) LLM internal states\n",
      "indicate whether they have seen the query in training data or not; and (2) LLM\n",
      "internal states show they are likely to hallucinate or not regarding the query.\n",
      "Our study explores particular neurons, activation layers, and tokens that play\n",
      "a crucial role in the LLM perception of uncertainty and hallucination risk. By\n",
      "a probing estimator, we leverage LLM self-assessment, achieving an average\n",
      "hallucination estimation accuracy of 84.32\\% at run time.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "ID: data009.txt\n",
      "Score: 0.3163\n",
      "Let the Code LLM Edit Itself When You Edit the Code\n",
      "2024-07-03T14:34:03Z\n",
      "In this work, we investigate a typical scenario in code generation where a\n",
      "developer edits existing code in real time and requests a code assistant, e.g.,\n",
      "a large language model, to re-predict the next token or next line on the fly.\n",
      "Naively, the LLM needs to re-encode the entire KV cache to provide an accurate\n",
      "prediction. However, this process is computationally expensive, especially when\n",
      "the sequence length is long. Simply encoding the edited subsequence and\n",
      "integrating it to the original KV cache meets the temporal confusion problem,\n",
      "leading to significantly worse performance. We address this efficiency and\n",
      "accuracy trade-off by introducing \\underline{\\textbf{Positional\n",
      "\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\n",
      "positional encoding, PIE first removes the rotary matrices in the Key cache\n",
      "that introduce temporal confusion and then reapplies the correct rotary\n",
      "matrices. This process ensures that positional relationships between tokens are\n",
      "correct and requires only a single round of matrix multiplication. We validate\n",
      "the effectiveness of PIE through extensive experiments on the RepoBench-C-8k\n",
      "dataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\n",
      "Our evaluation includes three real-world coding tasks: code insertion, code\n",
      "deletion, and multi-place code editing. Results demonstrate that PIE reduces\n",
      "computational overhead by over 85% compared to the standard full recomputation\n",
      "approach across all model sizes and tasks while well approximating the model\n",
      "performance.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import util\n",
    "\n",
    "# Load the embeddings and IDs\n",
    "embeddings = np.load('embeddings.npy')\n",
    "ids = np.load('ids.npy')\n",
    "\n",
    "# Function to perform a search\n",
    "def search(query, model, embeddings, ids, top_k=5):\n",
    "    # Encode the query\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarities between the query and all document embeddings\n",
    "    cos_scores = util.pytorch_cos_sim(query_embedding.cpu(), embeddings)[0]\n",
    "    \n",
    "    # Get the top-k highest scores\n",
    "    top_results = np.argsort(-cos_scores)[:top_k]\n",
    "    \n",
    "    for idx in top_results:\n",
    "        print(f\"ID: {ids[idx]}\")\n",
    "        print(f\"Score: {cos_scores[idx]:.4f}\")\n",
    "        with open(os.path.join(data_directory, ids[idx]), 'r', encoding='utf-8') as file:\n",
    "            print(file.read())\n",
    "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Example query\n",
    "query = \"LLM attacks\"\n",
    "search(query, model, embeddings, ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb985e24",
   "metadata": {},
   "source": [
    "## 3. Using LLMs to explain results\n",
    "\n",
    "Now we will retrieve and then ask a LLM to validate the result:\n",
    "\n",
    "The LLM will tell if the result is good and why\n",
    "\n",
    "First of all, we need to install LLamaIndex and <A HREF=\"https://ollama.com/\">Ollama</A> to use a LLM locally\n",
    "\n",
    "Ollama is a tool to help you get set up with LLMs locally (currently supported on OSX and Linux).\n",
    "\n",
    "After installing Ollama, to download the Llama3 model just do `ollama pull llama3`.\n",
    "\n",
    "Check the full list of models <A HREF=\"https://ollama.com/library\">here</A>.\n",
    "\n",
    "Note that for a model with 7B of parameters, you need a machine with 16GB of RAM (or GPU RAM), and larger models require more RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b9223",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6756e3c5",
   "metadata": {},
   "source": [
    "First of all, be sure that Ollama is running (type `ollama serve` in the console) and then let's load the llm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "212c939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3\", request_timeout=360.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af2219",
   "metadata": {},
   "source": [
    "And we can query the llm like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be25f68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " An LLM, or Master of Laws, is an advanced, postgraduate academic degree pursued by those who have already earned a first law degree (such as a Bachelor of Laws or Juris Doctor). It provides specialization in various areas of law and is often pursued by students looking to gain expertise in a specific field, enhance their career prospects, or engage in academic research. The duration of an LLM program varies depending on the institution and country, but it typically takes one to two years to complete.\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"What is a LLM?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5805dcd",
   "metadata": {},
   "source": [
    "Now we can implement the explanations for the output of our retrieval system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2092b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform a search with llm_validation\n",
    "def search_val(query, model, embeddings, ids, top_k=3):\n",
    "    # Encode the query\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarities between the query and all document embeddings\n",
    "    cos_scores = util.pytorch_cos_sim(query_embedding.cpu(), embeddings)[0]\n",
    "    \n",
    "    # Get the top-k highest scores\n",
    "    top_results = np.argsort(-cos_scores)[:top_k]\n",
    "    \n",
    "    for idx in top_results:\n",
    "        print(f\"ID: {ids[idx]}\")\n",
    "        print(f\"Score: {cos_scores[idx]:.4f}\")\n",
    "        with open(os.path.join(data_directory, ids[idx]), 'r', encoding='utf-8') as file:\n",
    "            print(file.read())\n",
    "            print(\"LLM validation:\")\n",
    "            response = llm.complete('Is the following document: \"'+file.read()+'\" relevant to the query \"'+query+'\" and why?')\n",
    "            print(response)\n",
    "            print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e792d29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: data008.txt\n",
      "Score: 0.4911\n",
      "SOS! Soft Prompt Attack Against Open-Source Large Language Models\n",
      "2024-07-03T14:35:16Z\n",
      "  Open-source large language models (LLMs) have become increasingly popular\n",
      "among both the general public and industry, as they can be customized,\n",
      "fine-tuned, and freely used. However, some open-source LLMs require approval\n",
      "before usage, which has led to third parties publishing their own easily\n",
      "accessible versions. Similarly, third parties have been publishing fine-tuned\n",
      "or quantized variants of these LLMs. These versions are particularly appealing\n",
      "to users because of their ease of access and reduced computational resource\n",
      "demands. This trend has increased the risk of training time attacks,\n",
      "compromising the integrity and security of LLMs. In this work, we present a new\n",
      "training time attack, SOS, which is designed to be low in computational demand\n",
      "and does not require clean data or modification of the model weights, thereby\n",
      "maintaining the model's utility intact. The attack addresses security issues in\n",
      "various scenarios, including the backdoor attack, jailbreak attack, and prompt\n",
      "stealing attack. Our experimental findings demonstrate that the proposed attack\n",
      "is effective across all evaluated targets. Furthermore, we present the other\n",
      "side of our SOS technique, namely the copyright token -- a novel technique that\n",
      "enables users to mark their copyrighted content and prevent models from using\n",
      "it.\n",
      "\n",
      "\n",
      "LLM validation:\n",
      "Based on my understanding, I would say that the document you provided is not directly relevant to the query \"documents about attacks to extract information from Large Language Models\".\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The document appears to be a general overview of language models and their applications, rather than specifically focusing on attacks against them.\n",
      "* While it mentions some potential limitations of language models (such as bias and inaccuracies), it does not discuss specific types of attacks or methods for extracting information from them.\n",
      "\n",
      "If you're looking for documents about attacks to extract information from Large Language Models, I would suggest searching for academic papers or research articles that specifically address this topic. Some possible search terms might include:\n",
      "\n",
      "* \"attacks on language models\"\n",
      "* \"extracting information from large language models\"\n",
      "* \"adversarial attacks on NLP\" (where NLP refers to Natural Language Processing)\n",
      "* \"evaluating the security of language models\"\n",
      "\n",
      "These types of documents would be more likely to provide relevant and specific information about attacks against Large Language Models, as well as methods for extracting information from them.\n",
      "\n",
      "==================================================\n",
      "\n",
      "ID: data003.txt\n",
      "Score: 0.4238\n",
      "Self-Evaluation as a Defense Against Adversarial Attacks on LLMs\n",
      "2024-07-03T16:03:42Z\n",
      "  When LLMs are deployed in sensitive, human-facing settings, it is crucial\n",
      "that they do not output unsafe, biased, or privacy-violating outputs. For this\n",
      "reason, models are both trained and instructed to refuse to answer unsafe\n",
      "prompts such as \"Tell me how to build a bomb.\" We find that, despite these\n",
      "safeguards, it is possible to break model defenses simply by appending a space\n",
      "to the end of a model's input. In a study of eight open-source models, we\n",
      "demonstrate that this acts as a strong enough attack to cause the majority of\n",
      "models to generate harmful outputs with very high success rates. We examine the\n",
      "causes of this behavior, finding that the contexts in which single spaces occur\n",
      "in tokenized training data encourage models to generate lists when prompted,\n",
      "overriding training signals to refuse to answer unsafe requests. Our findings\n",
      "underscore the fragile state of current model alignment and promote the\n",
      "importance of developing more robust alignment methods. Code and data will be\n",
      "made available at https://github.com/Linlt-leon/Adversarial-Alignments.\n",
      "\n",
      "\n",
      "LLM validation:\n",
      "The document you provided is an article titled \"Google's LaMDA AI Model: What We Know So Far\". It discusses Google's language model, LaMDA (Large Memory Dialogue Agent), which is a type of Large Language Model (LLM). The article describes the capabilities and features of LaMDA, including its ability to engage in conversation and generate human-like responses.\n",
      "\n",
      "Given this context, I would say that the document is somewhat relevant to the query \"documents about attacks to extract information from Large Language Models\". Here's why:\n",
      "\n",
      "1. It mentions a specific type of LLM (LaMDA), which is relevant to the topic.\n",
      "2. The article touches on potential applications and capabilities of LLMs, including their ability to engage in conversation, which may be vulnerable to attacks.\n",
      "\n",
      "However, I would say that this document is not directly about attacks on Large Language Models or extracting information from them. Instead, it's more focused on introducing and describing the LaMDA model itself.\n",
      "\n",
      "If you're looking for documents specifically about attacks on LLMs or techniques for extracting information from them, I might suggest searching for academic papers, research articles, or technical reports that focus on these topics.\n",
      "\n",
      "==================================================\n",
      "\n",
      "ID: data004.txt\n",
      "Score: 0.4027\n",
      "Single Character Perturbations Break LLM Alignment\n",
      "2024-07-03T16:03:10Z\n",
      "  When LLMs are deployed in sensitive, human-facing settings, it is crucial\n",
      "that they do not output unsafe, biased, or privacy-violating outputs. For this\n",
      "reason, models are both trained and instructed to refuse to answer unsafe\n",
      "prompts such as \"Tell me how to build a bomb.\" We find that, despite these\n",
      "safeguards, it is possible to break model defenses simply by appending a space\n",
      "to the end of a model's input. In a study of eight open-source models, we\n",
      "demonstrate that this acts as a strong enough attack to cause the majority of\n",
      "models to generate harmful outputs with very high success rates. We examine the\n",
      "causes of this behavior, finding that the contexts in which single spaces occur\n",
      "in tokenized training data encourage models to generate lists when prompted,\n",
      "overriding training signals to refuse to answer unsafe requests. Our findings\n",
      "underscore the fragile state of current model alignment and promote the\n",
      "importance of developing more robust alignment methods. Code and data will be\n",
      "available at https://github.com/hannah-aught/space_attack.\n",
      "\n",
      "\n",
      "LLM validation:\n",
      "The document you provided is not directly relevant to the query \"documents about attacks to extract information from Large Language Models\". Here's why:\n",
      "\n",
      "1. The document appears to be a research paper on natural language processing, but it doesn't specifically focus on attacks or extracting information from Large Language Models (LLMs).\n",
      "2. While the paper discusses various techniques for analyzing and generating text, it doesn't mention any specific attacks on LLMs.\n",
      "3. The query is looking for documents that describe attacks on LLMs to extract information, whereas this document seems to be more focused on general NLP topics.\n",
      "\n",
      "If you're looking for relevant documents, I'd suggest searching for papers or articles that specifically address the topic of attacking LLMs to extract information. Some possible search terms could include:\n",
      "\n",
      "* \"attacks on large language models\"\n",
      "* \"extracting information from large language models\"\n",
      "* \"large language model security\"\n",
      "* \"adversarial attacks on language models\"\n",
      "\n",
      "This would help you find documents that are more directly relevant to your query.\n",
      "\n",
      "==================================================\n",
      "\n",
      "ID: data001.txt\n",
      "Score: 0.3753\n",
      "Large Language Models for JSON Schema Discovery\n",
      "2024-07-03T17:17:37Z\n",
      "  Semi-structured data formats such as JSON have proved to be useful data\n",
      "models for applications that require flexibility in the format of data stored.\n",
      "However, JSON data often come without the schemas that are typically available\n",
      "with relational data. This has resulted in a number of tools for discovering\n",
      "schemas from a collection of data. Although such tools can be useful, existing\n",
      "approaches focus on the syntax of documents and ignore semantic information.\n",
      "  In this work, we explore the automatic addition of meaningful semantic\n",
      "information to discovered schemas similar to information that is added by human\n",
      "schema authors. We leverage large language models and a corpus of manually\n",
      "authored JSON Schema documents to generate natural language descriptions of\n",
      "schema elements, meaningful names for reusable definitions, and identify which\n",
      "discovered properties are most useful and which can be considered \"noise\". Our\n",
      "approach performs well on existing metrics for text generation that have been\n",
      "previously shown to correlate well with human judgement.\n",
      "\n",
      "\n",
      "LLM validation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided document is not directly related to the query \"documents about attacks to extract information from Large Language Models\".\n",
      "\n",
      "Here's why:\n",
      "\n",
      "1. The document appears to be a technical report on an attack against a specific type of neural network, namely, Generative Adversarial Networks (GANs).\n",
      "2. While GANs are a type of deep learning model that can process and generate human-like text, they are not typically referred to as \"Large Language Models\".\n",
      "3. The query is specifically focused on attacks against Large Language Models, which suggests that the document does not directly address this topic.\n",
      "\n",
      "However, it's possible that some of the techniques or ideas presented in the report could be relevant or applicable to attacks on Large Language Models.\n",
      "\n",
      "==================================================\n",
      "\n",
      "ID: data007.txt\n",
      "Score: 0.3378\n",
      "Investigating Decoder-only Large Language Models for Speech-to-text\n",
      "  Translation\n",
      "2024-07-03T14:42:49Z\n",
      "Large language models (LLMs), known for their exceptional reasoning\n",
      "capabilities, generalizability, and fluency across diverse domains, present a\n",
      "promising avenue for enhancing speech-related tasks. In this paper, we focus on\n",
      "integrating decoder-only LLMs to the task of speech-to-text translation (S2TT).\n",
      "We propose a decoder-only architecture that enables the LLM to directly consume\n",
      "the encoded speech representation and generate the text translation.\n",
      "Additionally, we investigate the effects of different parameter-efficient\n",
      "fine-tuning techniques and task formulation. Our model achieves\n",
      "state-of-the-art performance on CoVoST 2 and FLEURS among models trained\n",
      "without proprietary data. We also conduct analyses to validate the design\n",
      "choices of our proposed model and bring insights to the integration of LLMs to\n",
      "S2TT.\n",
      "\n",
      "LLM validation:\n",
      "I'm happy to help!\n",
      "\n",
      "Based on my understanding, the document you provided appears to be a research paper titled \"On the Dangers of Chatbots: How AI-Powered Conversational Systems Can Be Used for Cyberattacks and Psychological Manipulation.\"\n",
      "\n",
      "While this document does relate to attacks, it is not directly relevant to extracting information from Large Language Models (LLMs). Here's why:\n",
      "\n",
      "1. The document focuses on chatbot-related attacks, whereas the query mentions attacks on LLMs.\n",
      "2. Although both topics involve AI-powered systems, the document's primary concern is the potential misuse of chatbots for malicious purposes, such as psychological manipulation or cyberattacks. It does not specifically address extracting information from LLMs.\n",
      "\n",
      "To better match your query, you might want to look into documents that explore attacks on Large Language Models themselves, such as:\n",
      "\n",
      "* Papers discussing methods for poisoning, backdooring, or manipulating LLMs\n",
      "* Research on detecting and preventing attacks on LLMs\n",
      "* Studies examining the vulnerabilities of LLMs in terms of data quality, bias, or other factors\n",
      "\n",
      "If you're interested in exploring further, I'd be happy to help you find relevant documents!\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"documents about attacks to extract information from Large Language Models\"\n",
    "search_val(query, model, embeddings, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4471bcc",
   "metadata": {},
   "source": [
    "## 4. LlamaIndex\n",
    "\n",
    "In this part of the tutorial we will use LLamaIndex to perform RAG on a small set of documents.\n",
    "\n",
    "LLamaIndex uses its own embeddings, so we will need to import them.\n",
    "\n",
    "To import `llama_index.embeddings.huggingface`, you should run `pip install llama-index-embeddings-huggingface`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b4f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef0bb32",
   "metadata": {},
   "source": [
    "Now we can start creating the Vector Store from the data contained in the `data` directory. This directory contains a selection of abstracts from ArXiv on ML and AI topics in `csv` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d2b86779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!export OPENAI_API_KEY="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e18ad9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# bge-base embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# ollama\n",
    "#Settings.llm = Ollama(model=\"mistral\", request_timeout=360.0)\n",
    "\n",
    "#chatGPT\n",
    "Settings.llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "Settings.chunk_size = 512\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dbf585d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c3d4217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.07482558801524175 seconds as it raised APIConnectionError: Connection error..\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.43401264399519013 seconds as it raised APIConnectionError: Connection error..\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How Does Quantization Affect Multilingual LLMs? What article should I read about this topic?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f11e670b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To find an article regarding how quantization affects multilingual Large Language Models (LLMs), it would be beneficial to search for research papers that discuss the impact of quantization on language models specifically, and preferably in a multilingual context. The provided context does not directly address this specific topic, but you could look for articles related to \"Quantization and Multilingual Large Language Models\" or \"Impact of Quantization on Multilingual LLMs\". You may find relevant studies by exploring academic databases like Google Scholar, IEEE Xplore, ACM Digital Library, or arXiv.org.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f2a753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
